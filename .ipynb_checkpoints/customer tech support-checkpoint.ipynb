{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e57c3-c715-4f71-baca-6a3c228f6bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1069 documents\n",
      "âœ… Split into 1996 chunks\n",
      "âœ… Vector database created and saved!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit'):  Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Answer:\n",
      " The provided text appears to be a list of YANG Configuration Commands for Cisco devices. Here are some of the commands listed:\n",
      "\n",
      "1. alarms commands\n",
      "2. alias commands\n",
      "3. applications commands\n",
      "4. certificate commands\n",
      "5. cfm commands\n",
      "6. classifiers commands\n",
      "7. confdConfig commands\n",
      "8. cpu-load-control-cfg commands\n",
      "\n",
      "These commands can be used for various purposes such as managing alarms, creating aliases, configuring applications, managing certificates, controlling CPU load, etc., within a network device configuration context.\n",
      "\n",
      "ðŸ“š Sources:\n",
      "- userguide.pdf\n",
      "- userguide.pdf\n",
      "- userguide.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit'):  can you give me the lightspan RPC of Initiating Linktrace test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Answer:\n",
      " The Lightspan RPC for initiating a linktrace series test is as follows:\n",
      "\n",
      "```xml\n",
      "<rpc xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"1\">\n",
      "  <get>\n",
      "    <filter type=\"subtree\">\n",
      "      <cfm xmlns=\"urn:ieee:std:802.1Q:yang:ieee802-dot1q-cfm\">\n",
      "        <maintenance-group>\n",
      "          <maintenance-group-id>MG</maintenance-group-id>\n",
      "          <mep>\n",
      "            <mep-id>8191</mep-id>\n",
      "            <linktrace/>\n",
      "          </mep>\n",
      "        </maintenance-group>\n",
      "      </cfm>\n",
      "    </filter>\n",
      "  </get>\n",
      "</rpc>\n",
      "```\n",
      "\n",
      "ðŸ“š Sources:\n",
      "- CFM_OAM.md\n",
      "- CFM_OAM.md\n",
      "- CFM_OAM.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "DATA_DIR = \"data\"\n",
    "VECTOR_DB_DIR = \"embeddings\"\n",
    "OLLAMA_URL = \"http://host.docker.internal:11434\"   # Allow Docker to talk to host Ollama\n",
    "MODEL_NAME = \"mistral\"  # must match your installed Ollama model\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 1: LOAD DOCUMENTS\n",
    "# -------------------------------------------------\n",
    "docs = []\n",
    "pdf_loader = PyMuPDFLoader(\"userguide.pdf\")\n",
    "docs.extend(pdf_loader.load())\n",
    "\n",
    "md_loader = TextLoader(\"CFM_OAM.md\", encoding=\"utf-8\")\n",
    "docs.extend(md_loader.load())\n",
    "\n",
    "print(f\"âœ… Loaded {len(docs)} documents\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 2: SPLIT INTO CHUNKS\n",
    "# -------------------------------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"âœ… Split into {len(chunks)} chunks\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 3: CREATE EMBEDDINGS (CPU for reliability)\n",
    "# -------------------------------------------------\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=VECTOR_DB_DIR\n",
    ")\n",
    "vectordb.persist()\n",
    "print(\"âœ… Vector database created and saved!\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 4: CUSTOM OLLAMA LLM WRAPPER (Direct API)\n",
    "# -------------------------------------------------\n",
    "class OllamaLLM(LLM):\n",
    "    model: str = MODEL_NAME\n",
    "    api_url: str = OLLAMA_URL\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call the Ollama API directly\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/api/generate\",\n",
    "            json={\"model\": self.model, \"prompt\": prompt, \"stream\": False}\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            return data.get(\"response\", \"\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"âš ï¸ Invalid JSON response from Ollama:\")\n",
    "            print(response.text)\n",
    "            return \"\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model\": self.model}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama_api\"\n",
    "\n",
    "\n",
    "llm = OllamaLLM()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 5: BUILD RETRIEVAL CHAIN\n",
    "# -------------------------------------------------\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 6: INTERACTIVE LOOP\n",
    "# -------------------------------------------------\n",
    "while True:\n",
    "    query = input(\"\\nAsk a question (or type 'exit'): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    print(\"\\nðŸ§  Answer:\")\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    print(\"\\nðŸ“š Sources:\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(\"-\", doc.metadata.get(\"source\", \"Unknown file\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58205e72-7edd-4de3-bcb0-b668988c9e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: ollama: not found\n"
     ]
    }
   ],
   "source": [
    "!ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7892d28f-aa8a-45bc-bdfe-c83c9b9aa87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Model Output:\n",
      "\n",
      " Data normalization is a crucial pre-processing step in machine learning that aims to ensure all features or variables in a dataset are on a similar scale, reducing the impact of one feature dominating others and improving the performance and convergence speed of algorithms. Here's why it's important:\n",
      "\n",
      "1. Algorithm Fairness: Machine learning algorithms tend to perform better with data that is evenly distributed across different ranges. If one attribute has a larger range of values, the algorithm may focus more on this feature, neglecting others that might be equally important. Normalization eliminates this bias by scaling all features to a common range.\n",
      "\n",
      "2. Improved Learning: In some machine learning algorithms, especially those using Euclidean distance (like k-Nearest Neighbors or Support Vector Machines), the performance directly depends on the scale of features. Normalization ensures that the distances between data points are accurate and meaningful, improving the overall model's accuracy.\n",
      "\n",
      "3. Stability: Some algorithms like decision trees and neural networks are sensitive to large differences in the scales of their input variables. This could lead to unstable results or overfitting. By normalizing the data, we reduce these variations and make the models more robust and generalizable.\n",
      "\n",
      "4. Convergence Speed: Normalization can also speed up the learning process, especially for iterative algorithms like gradient descent. When features have large ranges, the learning rate must be reduced to prevent overshooting or undershooting during optimization. By normalizing the data, the learning rate can be increased, which accelerates convergence and reduces training time.\n",
      "\n",
      "5. Computational Efficiency: In some cases, normalization can lead to faster computations because small differences between similar values will have more significant effects on normalized data than on raw data. This is especially beneficial when dealing with large datasets or complex models.\n",
      "\n",
      "In summary, data normalization is an essential step in preparing data for machine learning algorithms. It ensures fairness, improves learning and convergence speed, increases stability, and promotes computational efficiency. However, it's important to remember that not all problems require normalization, and care should be taken when deciding whether or not to apply this pre-processing step.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "OLLAMA_URL = \"http://host.docker.internal:11434\"\n",
    "\n",
    "# Send the request\n",
    "response = requests.post(f\"{OLLAMA_URL}/api/generate\", json={\n",
    "    \"model\": \"mistral\",\n",
    "    \"prompt\": \"Explain the importance of data normalization in machine learning.\"\n",
    "}, stream=True)\n",
    "\n",
    "# Ollama streams multiple JSON chunks\n",
    "full_output = \"\"\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        data = json.loads(line.decode(\"utf-8\"))\n",
    "        if \"response\" in data:\n",
    "            full_output += data[\"response\"]\n",
    "        elif data.get(\"done\"):\n",
    "            break\n",
    "\n",
    "print(\"\\nðŸ§  Model Output:\\n\")\n",
    "print(full_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff5e46-0e95-42d4-b4be-487a645e13e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
