{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c6e57c3-c715-4f71-baca-6a3c228f6bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1431 documents\n",
      "‚úÖ Split into 3231 chunks\n",
      "‚úÖ Vector database created and saved!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit'):  Hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Answer:\n",
      " The provided context appears to be a list of acronyms and PDUs (Protocol Data Units) related to telecommunications, specifically in the context of Rec. ITU-T G.8013/Y.1731 (06/2023). Here's a list of some of them:\n",
      "\n",
      "1. Multicast destination addresses\n",
      "2. CCM - Congestion Control and Avoidance Mechanism\n",
      "3. LBM - Link Bandwidth Management\n",
      "4. LBR - Link Resource Manager\n",
      "5. LTM - Link Traffic Management\n",
      "6. LTR - Link Traffic Routing\n",
      "7. AIS - Alarm Indication Signal\n",
      "8. DMM - Downstream Multicast Management\n",
      "9. DMR - Downstream Multicast Reporting\n",
      "10. EXM - Explicit Multicast\n",
      "11. EXR - Explicit Unicast with RSVP\n",
      "12. VSM - Virtual Session Manager\n",
      "13. VSR - Virtual Service Registration\n",
      "14. CSF - Congestion Avoidance Signal\n",
      "15. SLM - Service Level Management\n",
      "\n",
      "In addition, there are several PDU types listed:\n",
      "\n",
      "1. LCK PDU - Link Control Protocol Data Unit\n",
      "2. TST PDU - Test Protocol Data Unit\n",
      "3. APS PDU - Advanced Peer-to-Peer Signaling Protocol Data Unit\n",
      "4. MCC PDU - Multicast Control Protocol Data Unit\n",
      "5. LMM PDU - Link Management Protocol Data Unit\n",
      "6. LMR PDU - Link Monitoring and Reporting Protocol Data Unit\n",
      "7. 1DM PDU - One-way Delay Measurement Protocol Data Unit\n",
      "8. DMM PDU - Downstream Multicast Management Protocol Data Unit\n",
      "\n",
      "üìö Sources:\n",
      "- y1731.pdf\n",
      "- y1731.pdf\n",
      "- y1731.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit'):  What is CFM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Answer:\n",
      " CFM, based on the provided context, stands for Connection-Oriented Control Function Mechanism. It's a concept used in network communications, specifically in Multi-Protocol Label Switching (MPLS). The CFM aims to provide a mechanism to support fast rerouting and restoration of MPLS connections in case of failures or congestion. It operates within the Management Plane of an MPLS network, using Protocol Data Units (PDUs) for communication between different management points (MPs). However, for achieving its full potential, hardware modifications to existing Provider Bridges might be required, as mentioned in the text.\n",
      "\n",
      "üìö Sources:\n",
      "- 8021ag-2007.pdf\n",
      "- 8021ag-2007.pdf\n",
      "- 8021ag-2007.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit'):  what are the Y1731 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Answer:\n",
      " Based on the provided context, the number 617 is repeated three times as the \"statically defined alarm type identifier.\" However, the context doesn't provide information about Y1731 features. The Y1731 is a standard for optical transport network alarms and faults, but without additional information, it's not possible to determine specific features related to this number 617.\n",
      "\n",
      "üìö Sources:\n",
      "- userguide.pdf\n",
      "- userguide.pdf\n",
      "- userguide.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question (or type 'exit'):  exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "DATA_DIR = \"data\"\n",
    "VECTOR_DB_DIR = \"embeddings\"\n",
    "OLLAMA_URL = \"http://host.docker.internal:11434\"   # Allow Docker to talk to host Ollama\n",
    "MODEL_NAME = \"mistral\"  # must match your installed Ollama model\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 1: LOAD DOCUMENTS\n",
    "# -------------------------------------------------\n",
    "docs = []\n",
    "pdf_loader = PyMuPDFLoader(\"userguide.pdf\")\n",
    "y1731loader = PyMuPDFLoader(\"y1731.pdf\")\n",
    "cfm_pdf = PyMuPDFLoader(\"8021ag-2007.pdf\")\n",
    "docs.extend(y1731loader.load())\n",
    "docs.extend(cfm_pdf.load())\n",
    "docs.extend(pdf_loader.load())\n",
    "\n",
    "md_loader = TextLoader(\"CFM_OAM.md\", encoding=\"utf-8\")\n",
    "docs.extend(md_loader.load())\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(docs)} documents\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 2: SPLIT INTO CHUNKS\n",
    "# -------------------------------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"‚úÖ Split into {len(chunks)} chunks\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 3: CREATE EMBEDDINGS (CPU for reliability)\n",
    "# -------------------------------------------------\n",
    "embedding_function = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cpu\"}\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=VECTOR_DB_DIR\n",
    ")\n",
    "vectordb.persist()\n",
    "print(\"‚úÖ Vector database created and saved!\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 4: CUSTOM OLLAMA LLM WRAPPER (Direct API)\n",
    "# -------------------------------------------------\n",
    "class OllamaLLM(LLM):\n",
    "    model: str = MODEL_NAME\n",
    "    api_url: str = OLLAMA_URL\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"Call the Ollama API directly\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.api_url}/api/generate\",\n",
    "            json={\"model\": self.model, \"prompt\": prompt, \"stream\": False}\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            data = response.json()\n",
    "            return data.get(\"response\", \"\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Invalid JSON response from Ollama:\")\n",
    "            print(response.text)\n",
    "            return \"\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model\": self.model}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama_api\"\n",
    "\n",
    "\n",
    "llm = OllamaLLM()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 5: BUILD RETRIEVAL CHAIN\n",
    "# -------------------------------------------------\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# STEP 6: INTERACTIVE LOOP\n",
    "# -------------------------------------------------\n",
    "while True:\n",
    "    query = input(\"\\nAsk a question (or type 'exit'): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    print(\"\\nüß† Answer:\")\n",
    "    print(result[\"result\"])\n",
    "\n",
    "    print(\"\\nüìö Sources:\")\n",
    "    for doc in result[\"source_documents\"]:\n",
    "        print(\"-\", doc.metadata.get(\"source\", \"Unknown file\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58205e72-7edd-4de3-bcb0-b668988c9e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: ollama: not found\n"
     ]
    }
   ],
   "source": [
    "!ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7892d28f-aa8a-45bc-bdfe-c83c9b9aa87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Model Output:\n",
      "\n",
      " Data normalization is a crucial pre-processing step in machine learning that aims to ensure all features or variables in a dataset are on a similar scale, reducing the impact of one feature dominating others and improving the performance and convergence speed of algorithms. Here's why it's important:\n",
      "\n",
      "1. Algorithm Fairness: Machine learning algorithms tend to perform better with data that is evenly distributed across different ranges. If one attribute has a larger range of values, the algorithm may focus more on this feature, neglecting others that might be equally important. Normalization eliminates this bias by scaling all features to a common range.\n",
      "\n",
      "2. Improved Learning: In some machine learning algorithms, especially those using Euclidean distance (like k-Nearest Neighbors or Support Vector Machines), the performance directly depends on the scale of features. Normalization ensures that the distances between data points are accurate and meaningful, improving the overall model's accuracy.\n",
      "\n",
      "3. Stability: Some algorithms like decision trees and neural networks are sensitive to large differences in the scales of their input variables. This could lead to unstable results or overfitting. By normalizing the data, we reduce these variations and make the models more robust and generalizable.\n",
      "\n",
      "4. Convergence Speed: Normalization can also speed up the learning process, especially for iterative algorithms like gradient descent. When features have large ranges, the learning rate must be reduced to prevent overshooting or undershooting during optimization. By normalizing the data, the learning rate can be increased, which accelerates convergence and reduces training time.\n",
      "\n",
      "5. Computational Efficiency: In some cases, normalization can lead to faster computations because small differences between similar values will have more significant effects on normalized data than on raw data. This is especially beneficial when dealing with large datasets or complex models.\n",
      "\n",
      "In summary, data normalization is an essential step in preparing data for machine learning algorithms. It ensures fairness, improves learning and convergence speed, increases stability, and promotes computational efficiency. However, it's important to remember that not all problems require normalization, and care should be taken when deciding whether or not to apply this pre-processing step.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "OLLAMA_URL = \"http://host.docker.internal:11434\"\n",
    "\n",
    "# Send the request\n",
    "response = requests.post(f\"{OLLAMA_URL}/api/generate\", json={\n",
    "    \"model\": \"mistral\",\n",
    "    \"prompt\": \"Explain the importance of data normalization in machine learning.\"\n",
    "}, stream=True)\n",
    "\n",
    "# Ollama streams multiple JSON chunks\n",
    "full_output = \"\"\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        data = json.loads(line.decode(\"utf-8\"))\n",
    "        if \"response\" in data:\n",
    "            full_output += data[\"response\"]\n",
    "        elif data.get(\"done\"):\n",
    "            break\n",
    "\n",
    "print(\"\\nüß† Model Output:\\n\")\n",
    "print(full_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34ff5e46-0e95-42d4-b4be-487a645e13e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1432 documents\n",
      "Split into 3251 chunks\n",
      "‚úÖ Vector DB created/persisted at embeddings\n",
      "\n",
      "Ready. Ask a question (type 'exit' to quit).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL ANSWER ===\n",
      "\n",
      " The command to display currently logged on users is \"who\". This command will mark the current session, which is running the show status command, with an asterisk.\n",
      "\n",
      "The information provided is from Document 1, 2, and 3 of userguide.pdf. Command Description: Display currently logged on users.The current session, i.e. the session running the show status command, is marked with an asterisk (1007)\n",
      "\n",
      "EXACT FROM CONTEXT:\n",
      "```\n",
      "who\n",
      "```\n",
      "\n",
      "====================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  What is CFM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL ANSWER ===\n",
      "\n",
      " Short direct answer: CFM, as mentioned in the provided documents, appears to be a technology or protocol related to Communications Framework Management (CFM). It involves generating and absorbing Control and Management (CCM) packets.\n",
      "\n",
      "Reasoning summary: The term \"CFM\" is repeatedly used throughout the context without an explicit definition. However, it can be inferred from the surrounding text that CFM refers to Communications Framework Management.\n",
      "\n",
      "Sources:\n",
      "- Document 1, line 16: generate and/or absorb these CCMs could be overwhelmed. To achieve its full potential, CFM could require hardware modifications to existing Provider Bridges.\n",
      "- Document 2, line 16: generate and/or absorb these CCMs could be overwhelmed. To achieve its full potential, CFM could require hardware modifications to existing Provider Bridges.\n",
      "- Document 3, line 16: generate and/or absorb these CCMs could be overwhelmed. To achieve its full potential, CFM could require hardware modifications to existing Provider Bridges.\n",
      "\n",
      "====================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  exit\n"
     ]
    }
   ],
   "source": [
    "# rag_with_cot_prompt.py\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATA_DIR = \"data\"\n",
    "VECTOR_DB_DIR = \"embeddings\"\n",
    "PDF_FILES = [\"userguide.pdf\", \"y1731.pdf\", \"8021ag-2007.pdf\"]\n",
    "MD_FILES = [\"CFM_OAM.md\", \"cfm-debugging.md\"]\n",
    "\n",
    "# Ollama host reachable from inside Docker:\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://host.docker.internal:11434\")\n",
    "OLLAMA_MODEL = \"mistral\"  # ensure this model exists in your local ollama\n",
    "\n",
    "# retrieval\n",
    "TOP_K = 3\n",
    "\n",
    "# ---------------- SYSTEM PROMPT (your COT policy) ----------------\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful, precise assistant specialized in using provided documents (CONTEXT) plus your internal knowledge when necessary. \n",
    "\n",
    "Rules:\n",
    "1. ALWAYS consult the CONTEXT block first for factual answers. If the CONTEXT contains explicit text that answers the question, answer only from that information.\n",
    "2. If the CONTEXT is insufficient or the user asks for theoretical/explanatory content, you may use your internal knowledge to answer ‚Äî but mark which parts come from CONTEXT and which parts are from your internal knowledge.\n",
    "3. Do NOT reveal raw internal chain-of-thought. Instead provide a brief \"Reasoning summary\" (2‚Äì4 lines) that explains the key steps or assumptions you used to reach the conclusion.\n",
    "4. For any command / RPC / exact-value request: prefer exact context matches. If the exact value or command is not present in CONTEXT, reply with \"NOT FOUND IN CONTEXT\" and then, only if the user asked to, provide a best-effort answer using internal knowledge labeled as such.\n",
    "5. When you cite CONTEXT, include the document id or filename and a short quote or line reference.\n",
    "6. When you produce code, commands, or RPC responses, return them in fenced code blocks and mark them clearly as `EXACT FROM CONTEXT` if pulled verbatim; otherwise label as `DERIVED` or `INTERNAL_KNOWLEDGE`.\n",
    "7. Also use multiple context when producing RPC and LightSpan RPC to give correct RPC, Check it multiple times majorly when it is asked to fetch RPC\n",
    "8. Do not disclose the internal documents in response.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------- Helper: build prompt ----------------\n",
    "def build_prompt(retrieved_docs: List, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Compose the full prompt with CONTEXT (verbatim retrieved passages),\n",
    "    followed by the user's question and instructions for the model.\n",
    "    \"\"\"\n",
    "    ctx_parts = []\n",
    "    for i, d in enumerate(retrieved_docs, start=1):\n",
    "        src = d.metadata.get(\"source\", f\"doc_{i}\")\n",
    "        snippet = d.page_content.strip()\n",
    "        # keep each snippet verbatim and include small excerpt label\n",
    "        ctx_parts.append(f\"[Document {i}: {src}]\\n{snippet}\\n\")\n",
    "\n",
    "    ctx_block = \"\\n\\n\".join(ctx_parts)\n",
    "    prompt = (\n",
    "        f\"{SYSTEM_PROMPT}\\n\\n\"\n",
    "        \"CONTEXT:\\n\"\n",
    "        \"========\\n\"\n",
    "        f\"{ctx_block}\\n\"\n",
    "        \"END_CONTEXT\\n\"\n",
    "        \"========\\n\\n\"\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"1) Use the CONTEXT above as primary source for factual answers. If CONTEXT contains the direct answer, use it verbatim and label EXACT FROM CONTEXT.\\n\"\n",
    "        \"2) If the CONTEXT does not contain an answer, you may answer using internal knowledge, but label that content as INTERNAL_KNOWLEDGE.\\n\"\n",
    "        \"3) Provide:\\n\"\n",
    "        \"   a) A short direct answer (1-3 sentences).\\n\"\n",
    "        \"   b) If an exact command or RPC is requested and is found verbatim in the CONTEXT, show it in a fenced code block labeled EXACT FROM CONTEXT. If not present, print NOT FOUND IN CONTEXT.\\n\"\n",
    "        \"   c) A short 'Reasoning summary' (2-4 lines). Do NOT reveal chain-of-thought.\\n\"\n",
    "        \"   d) A 'Sources' list referencing the document names and quoted snippets.\\n\\n\"\n",
    "        f\"QUESTION:\\n{question}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# ---------------- Ollama calling util ----------------\n",
    "def call_ollama(prompt: str, model: str = OLLAMA_MODEL, stream: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Call the local Ollama generate API.\n",
    "    If stream=False we expect a single JSON response; if stream=True we process JSONL.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_HOST}/api/generate\"\n",
    "    payload = {\"model\": model, \"prompt\": prompt, \"stream\": stream}\n",
    "    # Use a small timeout but can be increased for long responses\n",
    "    resp = requests.post(url, json=payload, stream=stream, timeout=300)\n",
    "\n",
    "    if stream:\n",
    "        # stream mode: iterate JSONL lines and accumulate 'response' fields\n",
    "        out = \"\"\n",
    "        for line in resp.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "            data = json.loads(line.decode(\"utf-8\"))\n",
    "            # Many Ollama responses include {\"response\": \"...\", ...}\n",
    "            if \"response\" in data and data[\"response\"]:\n",
    "                out += data[\"response\"]\n",
    "            if data.get(\"done\"):\n",
    "                break\n",
    "        return out\n",
    "    else:\n",
    "        # non-streaming: single JSON object\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except Exception as e:\n",
    "            # show raw text when JSON parse fails\n",
    "            print(\"Ollama response (raw):\", resp.text[:1000])\n",
    "            raise\n",
    "        return data.get(\"response\", \"\")\n",
    "\n",
    "# ---------------- Main flow ----------------\n",
    "def main():\n",
    "    # Load docs\n",
    "    docs = []\n",
    "    for PDF_FILE in PDF_FILES:\n",
    "        if os.path.exists(PDF_FILE):\n",
    "            pdf_loader = PyMuPDFLoader(PDF_FILE)\n",
    "            docs.extend(pdf_loader.load())\n",
    "    for MD_FILE in MD_FILES:\n",
    "        if os.path.exists(MD_FILE):\n",
    "            md_loader = TextLoader(MD_FILE, encoding=\"utf-8\")\n",
    "            docs.extend(md_loader.load())\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "    # Split\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "    # Embeddings (CPU) and Chroma vector DB\n",
    "    embedding_function = SentenceTransformerEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cpu\"}\n",
    "    )\n",
    "\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_function,\n",
    "        persist_directory=VECTOR_DB_DIR\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    print(\"‚úÖ Vector DB created/persisted at\", VECTOR_DB_DIR)\n",
    "\n",
    "    # create a retriever\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "    # interactive loop\n",
    "    print(\"\\nReady. Ask a question (type 'exit' to quit).\")\n",
    "    while True:\n",
    "        q = input(\"\\nQuestion: \").strip()\n",
    "        if q.lower() in (\"exit\", \"quit\"):\n",
    "            break\n",
    "\n",
    "        # retrieve top-k passages\n",
    "        retrieved = retriever.get_relevant_documents(q)\n",
    "        # build prompt that enforces your chain-of-thought policy\n",
    "        prompt = build_prompt(retrieved, q)\n",
    "\n",
    "        # call ollama (non-streaming for simplicity; set stream=True to stream)\n",
    "        try:\n",
    "            answer = call_ollama(prompt, model=OLLAMA_MODEL, stream=False)\n",
    "        except Exception as e:\n",
    "            print(\"Error calling Ollama:\", e)\n",
    "            continue\n",
    "\n",
    "        # print the model output\n",
    "        print(\"\\n=== MODEL ANSWER ===\\n\")\n",
    "        print(answer)\n",
    "        print(\"\\n====================\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fb8de5e-9cb2-4144-9fe7-36e4ac8aafb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.120.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting starlette<0.50.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.49.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from fastapi) (2.12.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from fastapi) (4.15.0)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi)\n",
      "  Downloading annotated_doc-0.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/conda/lib/python3.10/site-packages (from starlette<0.50.0,>=0.40.0->fastapi) (4.11.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Downloading fastapi-0.120.3-py3-none-any.whl (108 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m861.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading annotated_doc-0.0.3-py3-none-any.whl (5.5 kB)\n",
      "Downloading starlette-0.49.1-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.2/74.2 kB\u001b[0m \u001b[31m92.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: annotated-doc, starlette, fastapi\n",
      "Successfully installed annotated-doc-0.0.3 fastapi-0.120.3 starlette-0.49.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47d237ed-0349-4774-abe9-b9bcf6e30012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m16268\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "/app/COT_RAG.py:109: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedder = SentenceTransformerEmbeddings(\n",
      "Creating new Chroma DB at embeddings\n",
      "/app/COT_RAG.py:122: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n",
      "Vector DB ready ‚Äì 3251 chunks, top_k=3\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "^C\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "Shutting down...\n"
     ]
    }
   ],
   "source": [
    "!python COT_RAG.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1b43e30-06b5-4db2-bb0f-f878cbf0bc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: curl: not found\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://localhost:8000/query -H \"Content-Type: application/json\" -d '{\"question\":\"What is the command to enable CFM?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b2821c-5050-4f6d-8f2e-28bbedade9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
