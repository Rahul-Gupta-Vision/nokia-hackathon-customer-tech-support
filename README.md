# ğŸ§  Customer Tech Support Assistant
**AI-Powered Knowledge Retrieval and Chat System**

The **Customer Tech Support Assistant** is an intelligent AI-powered chatbot designed to assist both **customers** and **tech support engineers** by retrieving relevant information from internal documents and providing accurate, context-aware responses using **Mistral LLM** served through **Ollama**.

---

## ğŸ—ï¸ System Architecture


The system is composed of the following layers:

- **User Layer** â€“ Interfaces for customers and tech support engineers.  
- **Frontend Layer** â€“ React-based chat interface with login and chat windows.  
- **Backend Layer** â€“ Python REST API that handles queries, embeddings, and response generation.  
- **AI Model Layer** â€“ Mistral LLM served locally via Ollama for generating intelligent responses.  
- **Knowledge Layer** â€“ ChromaDB vector store holding embeddings of domain-specific documents.  

---

## âš™ï¸ Setup and Run Instructions

Follow these steps to set up and run the complete application locally:

### ğŸ§© 1. Install Ollama and Download the Mistral Model
Download and install Ollama from the [official website](https://ollama.ai/).  
Then, pull the **Mistral model**:

```bash
ollama pull mistral
```

---

### ğŸš€ 2. Start Ollama Server

Serve the model locally:

```bash
ollama serve
```

This will run the Ollama model service locally for inference.

---

### ğŸ 3. Set Up and Run the Backend

Install Python dependencies:

```bash
pip install -r req.txt
```

Then run the backend server:

```bash
python COT_RAG.py
```

This starts the REST API that handles query processing, vector embeddings, and model interaction.

---

### ğŸ’» 4. Set Up and Run the Frontend

Navigate to the frontend directory:

```bash
cd cust-tech-support
```

Make sure **Node.js** is installed. Then install dependencies and start the app:

```bash
npm install
npm run dev
```

---

### ğŸŒ 5. Access the Application

Once both backend and frontend are running, open your browser and go to:

ğŸ‘‰ **http://localhost:5173**

Youâ€™ll see the login window â€” after authentication, you can start interacting with the AI assistant via the chat window.

---

## ğŸ§  Features

- ğŸ” **AI-Powered Query Resolution** â€” Answers technical and troubleshooting queries.  
- ğŸ§© **Vector-Based Knowledge Retrieval** â€” Fetches relevant context from stored embeddings using **ChromaDB**.  
- ğŸ—‚ï¸ **Knowledge Base Categorization** â€” Organized into Provisioning, Troubleshooting, Standards, and Historical Issues KBs.  
- âš¡ **Real-Time Responses** â€” Seamless frontend-backend communication via REST API.  
- ğŸ” **Authenticated Access** â€” Separate roles for Customers and Support Engineers.

---

## ğŸ§± Tech Stack

| Layer | Technology |
|-------|-------------|
| **Frontend** | React.js, Vite, TailwindCSS |
| **Backend** | Python (Flask / FastAPI) |
| **AI Model Layer** | Ollama + Mistral LLM |
| **Vector Database** | ChromaDB |
| **Data Sources** | Provisioning Docs, Troubleshooting Docs, Standards, JIRA DB |
| **Package Manager** | npm, pip |

---

## ğŸ”® Future Enhancements

- ğŸ”— Integrate live JIRA issue retrieval and status updates.  
- ğŸ“Š Add analytics dashboard for query patterns and response accuracy.  
- ğŸ§  Implement feedback-based continual model improvement.  
- ğŸ§¾ Extend support for multi-language queries and responses.  
- ğŸ§‘â€ğŸ’¼ Add role-based permissions and team chat features.

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€” free to use and modify.
